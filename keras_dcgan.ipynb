{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' create dataset, \n",
    "select random n characters from a unicode range\n",
    "create images from selected characters, these will be used in minibatches\n",
    "'''\n",
    "\n",
    "import numpy as np\n",
    "from PIL import ImageFont, ImageDraw, Image\n",
    "from skimage.color import rgb2gray\n",
    "\n",
    "\n",
    "font_file = r\"c:\\windows\\fonts\\code2000.ttf\"\n",
    "font_size_offset = 1\n",
    "image_size_offset = (1,1)\n",
    "\n",
    "\n",
    "def get_best_font_size(image):\n",
    "    global font_size_offset, image_size_offset\n",
    "    \n",
    "    if font_size_offset != 1 and image_size_offset == image.size:\n",
    "        return font_size_offset\n",
    "    \n",
    "    # Iterate through different font sizes to find best fit\n",
    "    fontsize = 1\n",
    "    txt = \"æ—¥\"\n",
    "\n",
    "    font = ImageFont.truetype(font_file, fontsize)\n",
    "    while font.getsize(txt)[0] < image.size[0] and font.getsize(txt)[1] < image.size[1]:\n",
    "        fontsize += 1\n",
    "        font = ImageFont.truetype(font_file, fontsize)\n",
    "        \n",
    "    font_size_offset = fontsize\n",
    "    image_size_offset = image.size\n",
    "    \n",
    "    return font_size_offset\n",
    "\n",
    "\n",
    "def image_from_txt(txt, x, y, binary_img = False):\n",
    "    image = Image.new(\"L\", (x, y), (255))\n",
    "    draw = ImageDraw.Draw(image)\n",
    "    \n",
    "    fontsize = get_best_font_size(image)\n",
    "    font = ImageFont.truetype(font_file, fontsize)\n",
    "    \n",
    "    horiz_pos = (x - font.getsize(txt)[0])/2\n",
    "    vert_pos = (y - font.getsize(txt)[1])/2 - y/10\n",
    "\n",
    "    draw.text((horiz_pos, vert_pos), txt, font=font, fill = (0))\n",
    "    \n",
    "    # normalize and set dimensions appropriate for conv layers\n",
    "    result = (np.array(image)[:, :, np.newaxis] / 127.5) -1\n",
    "\n",
    "    # easier to train without grays\n",
    "    if binary_img:\n",
    "        result[result >= 0] = 1\n",
    "        result[result < 0] = -1\n",
    "    \n",
    "    # randomly flip images to artifically create more data\n",
    "    if np.random.rand() < 0.2:\n",
    "        return np.fliplr(result)\n",
    "    return result\n",
    "\n",
    "\n",
    "unicode_range = [\n",
    "    (0x3400, 0x4dbf), # CJK unified ideographs (Common and Uncommon Kanji)\n",
    "    (0x4e00, 0x9faf) # CJK unified ideographs extension A (Rare Kanji)\n",
    "]\n",
    "\n",
    "available_chars = [c for r in unicode_range for c in range(r[0], r[1]+1)]\n",
    "n_chars = len(available_chars)\n",
    "\n",
    "def get_random_kanji_image(n=1, x=64, y=64):\n",
    "    # randomly select kanjis within range\n",
    "    kanji_ranges = np.random.choice(available_chars, n)\n",
    "    \n",
    "    # create normalized numpy arrays from selected kanjis\n",
    "    return np.asarray([image_from_txt(chr(txt), x ,y) for txt in kanji_ranges])\n",
    "\n",
    "# generating training images on the run hurts performance over epochs, \n",
    "# so if memory is not a problem generate all at once and load to memory\n",
    "# kanji_image_set = get_random_kanji_image(30000)\n",
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "kanji_image_set = x_train[:30000]\n",
    "\n",
    "from scipy.misc import imresize\n",
    "kanji_image_set = np.array([imresize(i, (64,64)) for i in kanji_image_set])\n",
    "kanji_image_set = (kanji_image_set[:, :, :, np.newaxis] / 127.5) -1\n",
    "kanji_image_set[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check images are correctly created\n",
    "%matplotlib notebook\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image = get_random_kanji_image().squeeze()\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(image, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import (Sequential)\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.core import (Reshape, Dense, Flatten, Activation, Dropout)\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import (Conv2D, Conv2DTranspose)\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import plot_model\n",
    "\n",
    "\n",
    "# Set network & training parameters\n",
    "learning_rate = 0.0002\n",
    "batch_size = 64\n",
    "n_training_samples = n_chars\n",
    "training_epochs = 25\n",
    "display_step = 1\n",
    "\n",
    "output_folder = 'dcgan_results/'\n",
    "\n",
    "def set_trainable(model, is_trainable):\n",
    "    for l in model.layers:\n",
    "        l.trainable = is_trainable\n",
    "    model.trainable = is_trainable\n",
    "\n",
    "# Build generator model\n",
    "g = Sequential()\n",
    "\n",
    "# 100 -> 8192 (4*4 is because of 4 deconvolutions to reach 64)\n",
    "g.add(Dense(input_dim=100, units=64*8*4*4))\n",
    "g.add(BatchNormalization())\n",
    "g.add(LeakyReLU())\n",
    "\n",
    "# 8192 -> (4 x 4 x 512)\n",
    "g.add(Reshape((4, 4, 512)))\n",
    "g.add(Dropout(.5))\n",
    "\n",
    "# (4 x 4 x 512) -> (8 x 8 x 256)\n",
    "g.add(Conv2DTranspose(256, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "g.add(BatchNormalization())\n",
    "g.add(LeakyReLU())\n",
    "g.add(Dropout(.5))\n",
    "\n",
    "# (8 x 8 x 256) -> (16 x 16 x 128)\n",
    "g.add(Conv2DTranspose(128, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "g.add(BatchNormalization())\n",
    "g.add(LeakyReLU())\n",
    "g.add(Dropout(.5))\n",
    "\n",
    "# (16 x 16 x 128) -> (32 x 32 x 64)\n",
    "g.add(Conv2DTranspose(64, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "g.add(BatchNormalization())\n",
    "g.add(LeakyReLU())\n",
    "g.add(Dropout(.5))\n",
    "\n",
    "# (32 x 32 x 64) -> (64 x 64 x 1)\n",
    "g.add(Conv2DTranspose(1, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "g.add(Activation('tanh'))\n",
    "g.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate))\n",
    "\n",
    "\n",
    "# Build discriminator model\n",
    "d = Sequential()\n",
    "\n",
    "# (64 x 64 x 1) -> (32 x 32 x 64)\n",
    "d.add(Conv2D(64, (5, 5), strides=(2, 2), padding=\"same\", input_shape=(64, 64, 1)))\n",
    "d.add(BatchNormalization())\n",
    "d.add(LeakyReLU(alpha=.2))\n",
    "d.add(Dropout(.5))\n",
    "\n",
    "# (32 x 32 x 64) -> (16 x 16 x 128)\n",
    "d.add(Conv2D(128, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "d.add(BatchNormalization())\n",
    "d.add(LeakyReLU(alpha=.2))\n",
    "d.add(Dropout(.5))\n",
    "\n",
    "# (16 x 16 x 128) -> (8 x 8 x 256)\n",
    "d.add(Conv2D(256, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "d.add(BatchNormalization())\n",
    "d.add(LeakyReLU(alpha=.2))\n",
    "d.add(Dropout(.5))\n",
    "\n",
    "# (8 x 8 x 256) -> (4 x 4 x 512)\n",
    "d.add(Conv2D(512, (5, 5), strides=(2, 2), padding=\"same\"))\n",
    "d.add(BatchNormalization())\n",
    "d.add(LeakyReLU(alpha=.2))\n",
    "d.add(Dropout(.5))\n",
    "\n",
    "# (4 x 4 x 512) -> 8192\n",
    "d.add(Flatten())\n",
    "\n",
    "# 8192 -> 1\n",
    "d.add(Dense(1))\n",
    "d.add(Activation('sigmoid'))\n",
    "d.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate/2))\n",
    "set_trainable(d, False)\n",
    "\n",
    "\n",
    "# Combine models -> pred = discriminator(generator(input_noise))\n",
    "model = Sequential()\n",
    "model.add(g)\n",
    "model.add(d)\n",
    "model.compile(loss='binary_crossentropy', optimizer=Adam(lr=learning_rate))\n",
    "\n",
    "\n",
    "# save model graphs to disk\n",
    "plot_model(g, to_file=output_folder + 'model_generator.png', show_shapes=True)\n",
    "plot_model(d, to_file=output_folder + 'model_discriminator.png', show_shapes=True)\n",
    "plot_model(model, to_file=output_folder + 'model.png', show_shapes=True)\n",
    "\n",
    "\n",
    "model_stats = {'d_loss': [], 'g_loss': []}     \n",
    "for epoch in range(training_epochs):\n",
    "    total_batches = int(n_training_samples/batch_size);\n",
    "    avg_d_loss, avg_g_loss = 0, 0\n",
    "    for i in range(total_batches):\n",
    "        \n",
    "        # Generate images\n",
    "        # input_img = get_random_kanji_image(batch_size)\n",
    "        input_img = kanji_image_set[i*batch_size: (i+1)*batch_size]\n",
    "\n",
    "        input_noise = np.random.uniform(-1, 1, (batch_size, 100))\n",
    "        generated_img = g.predict(input_noise)\n",
    "        \n",
    "        x_d = np.concatenate([input_img, generated_img])\n",
    "        y_d = np.zeros(batch_size*2)\n",
    "        y_d[:batch_size] = 1\n",
    "        \n",
    "        # shuffle training set\n",
    "        p = np.random.permutation(batch_size*2)\n",
    "        x_d, y_d = x_d[p], y_d[p]\n",
    "        \n",
    "        # Train discriminator model\n",
    "        set_trainable(d, True)\n",
    "        d_loss = d.train_on_batch(x_d, y_d)\n",
    "        avg_d_loss += d_loss / total_batches\n",
    "        \n",
    "        # Train generator model\n",
    "        noise = np.random.uniform(-1, 1, (batch_size, 100))\n",
    "        y_g = np.ones(batch_size)\n",
    "        \n",
    "        set_trainable(d, False)\n",
    "        g_loss = model.train_on_batch(noise, y_g)\n",
    "        avg_g_loss += g_loss / total_batches\n",
    "        \n",
    "        # display current process\n",
    "        print(\"Epoch({}) - {}/{} | \".format(epoch, i+1,total_batches) + \n",
    "              \"#\" * round((i+1)*70/total_batches) + \n",
    "              \"-\" * round((total_batches-(i+1))*70/total_batches) + \n",
    "              \" | losses g:{:.6f},  d:{:.6f}\".format(g_loss, d_loss), \n",
    "              end=\"\\r\")\n",
    "    \n",
    "        # log losses overtime\n",
    "        model_stats['g_loss'].append(g_loss)\n",
    "        model_stats['d_loss'].append(d_loss)\n",
    "    \n",
    "    # display epoch results\n",
    "    if epoch % display_step == 0:\n",
    "        print(\"\\n\\tEpoch({}) - avg discriminator loss: {:.8f}, avg generator loss: {:.8f}\"\n",
    "              .format(epoch, avg_d_loss, avg_g_loss))\n",
    "        \n",
    "        # save intermediate epoch results to file\n",
    "        epoch_results = np.squeeze(g.predict(np.random.uniform(-1, 1, (4, 100))))\n",
    "        epoch_results += 1 \n",
    "        epoch_results *= 127.5 \n",
    "        \n",
    "        im = Image.fromarray(np.hstack(epoch_results)).convert(\"RGB\")\n",
    "        im.save(output_folder + \"epoch_{}.jpg\".format(epoch))\n",
    "        \n",
    "        # save model and its weights\n",
    "        d.save(output_folder + 'd_model_epoch_10.h5')\n",
    "        g.save(output_folder + 'g_model_epoch_10.h5')\n",
    "        model.save(output_folder + 'model_model_epoch_10.h5')\n",
    "\n",
    "\n",
    "# plot loss curves\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.plot(model_stats['g_loss'], c='r', linestyle='-', label='Generator')\n",
    "ax.plot(model_stats['d_loss'], c='b', linestyle='--', label='Discriminator')\n",
    "\n",
    "ax.set_xlabel('Batch Gradient Update')\n",
    "ax.set_ylabel('Loss')\n",
    "ax.set_title('DCGAN - Keras Training Results')\n",
    "ax.legend(loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# plot final image generation results\n",
    "n_examples = (4, 4)\n",
    "\n",
    "noise = np.random.uniform(-1, 1, (n_examples[0] * n_examples[1], 100))\n",
    "result = g.predict(noise)\n",
    "\n",
    "result += 1 \n",
    "result *= 127.5\n",
    "\n",
    "fig, axs = plt.subplots(*n_examples)\n",
    "\n",
    "i=0\n",
    "for ax in axs.flat:\n",
    "    ax.imshow(np.squeeze(result[i]), cmap=\"gray\")\n",
    "    ax.tick_params(bottom='off', top='off', left='off', right='off', \n",
    "                   labelbottom='off', labelleft='off')\n",
    "    i+=1\n",
    "    \n",
    "plt.subplots_adjust(wspace=0, hspace=0)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
